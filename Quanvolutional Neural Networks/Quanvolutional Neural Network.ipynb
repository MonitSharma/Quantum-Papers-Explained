{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quanvolutional Neural Networks {#quanvolution}\n",
        "==============================\n",
        "\n",
        "\n",
        "\n",
        "Introduction\n",
        "------------\n",
        "\n",
        "### Classical convolution\n",
        "\n",
        "The *convolutional neural network* (CNN) is a standard model in\n",
        "classical machine learning which is particularly suitable for processing\n",
        "images. The model is based on the idea of a *convolution layer* where,\n",
        "instead of processing the full input data with a global function, a\n",
        "local convolution is applied.\n",
        "\n",
        "If the input is an image, small local regions are sequentially processed\n",
        "with the same kernel. The results obtained for each region are usually\n",
        "associated to different channels of a single output pixel. The union of\n",
        "all the output pixels produces a new image-like object, which can be\n",
        "further processed by additional layers.\n",
        "\n",
        "### Quantum convolution\n",
        "\n",
        "One can extend the same idea also to the context of quantum variational\n",
        "circuits. A possible approach is given by the following procedure which\n",
        "is very similar to the one used in Ref. \\[1\\]. The scheme is also\n",
        "represented in the figure at the top of this tutorial.\n",
        "\n",
        "1.  A small region of the input image, in our example a $2 \\times 2$\n",
        "    square, is embedded into a quantum circuit. In this demo, this is\n",
        "    achieved with parametrized rotations applied to the qubits\n",
        "    initialized in the ground state.\n",
        "2.  A quantum computation, associated to a unitary $U$, is performed on\n",
        "    the system. The unitary could be generated by a variational quantum\n",
        "    circuit or, more simply, by a random circuit as proposed in Ref.\n",
        "    \\[1\\].\n",
        "3.  The quantum system is finally measured, obtaining a list of\n",
        "    classical expectation values. The measurement results could also be\n",
        "    classically post-processed as proposed in Ref. \\[1\\] but, for\n",
        "    simplicity, in this demo we directly use the raw expectation values.\n",
        "4.  Analogously to a classical convolution layer, each expectation value\n",
        "    is mapped to a different channel of a single output pixel.\n",
        "5.  Iterating the same procedure over different regions, one can scan\n",
        "    the full input image, producing an output object which will be\n",
        "    structured as a multi-channel image.\n",
        "6.  The quantum convolution can be followed by further quantum layers or\n",
        "    by classical layers.\n",
        "\n",
        "The main difference with respect to a classical convolution is that a\n",
        "quantum circuit can generate highly complex kernels whose computation\n",
        "could be, at least in principle, classically intractable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "General setup\n",
        "-------------\n",
        "\n",
        "This Python code requires *PennyLane* with the *TensorFlow* interface\n",
        "and the plotting library *matplotlib*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
            "Collecting tensorflow-intel==2.12.0\n",
            "  Downloading tensorflow_intel-2.12.0-cp311-cp311-win_amd64.whl (272.9 MB)\n",
            "     ------------------------------------- 272.9/272.9 MB 10.2 MB/s eta 0:00:00\n",
            "Collecting absl-py>=1.0.0\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "     ---------------------------------------- 126.5/126.5 kB ? eta 0:00:00\n",
            "Collecting astunparse>=1.6.0\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "     ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
            "Collecting h5py>=2.9.0\n",
            "  Downloading h5py-3.8.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
            "     ---------------------------------------- 2.6/2.6 MB 18.4 MB/s eta 0:00:00\n",
            "Collecting jax>=0.3.15\n",
            "  Downloading jax-0.4.12.tar.gz (1.3 MB)\n",
            "     ---------------------------------------- 1.3/1.3 MB 16.5 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting libclang>=13.0.0\n",
            "  Downloading libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
            "     ---------------------------------------- 24.4/24.4 MB 9.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "     ---------------------------------------- 65.5/65.5 kB ? eta 0:00:00\n",
            "Requirement already satisfied: packaging in c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.1)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
            "  Downloading protobuf-4.23.3-cp310-abi3-win_amd64.whl (422 kB)\n",
            "     ------------------------------------- 422.5/422.5 kB 12.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.1008.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.5.0)\n",
            "Collecting wrapt<1.15,>=1.11.0\n",
            "  Downloading wrapt-1.14.1.tar.gz (50 kB)\n",
            "     ---------------------------------------- 50.9/50.9 kB 2.5 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.54.2-cp311-cp311-win_amd64.whl (4.1 MB)\n",
            "     ---------------------------------------- 4.1/4.1 MB 7.9 MB/s eta 0:00:00\n",
            "Collecting tensorboard<2.13,>=2.12\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "     ---------------------------------------- 5.6/5.6 MB 16.4 MB/s eta 0:00:00\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "     ------------------------------------- 440.7/440.7 kB 13.5 MB/s eta 0:00:00\n",
            "Collecting keras<2.13,>=2.12.0\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "     ---------------------------------------- 1.7/1.7 MB 15.8 MB/s eta 0:00:00\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
            "     ---------------------------------------- 1.5/1.5 MB 15.7 MB/s eta 0:00:00\n",
            "Collecting wheel<1.0,>=0.23.0\n",
            "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
            "Collecting ml-dtypes>=0.1.0\n",
            "  Downloading ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl (938 kB)\n",
            "     ------------------------------------- 938.7/938.7 kB 15.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scipy>=1.7 in c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.10.1)\n",
            "Collecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.20.0-py2.py3-none-any.whl (181 kB)\n",
            "     ------------------------------------- 181.5/181.5 kB 10.7 MB/s eta 0:00:00\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
            "     ---------------------------------------- 93.9/93.9 kB ? eta 0:00:00\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.30.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
            "     ------------------------------------- 242.5/242.5 kB 15.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.1)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "     ---------------------------------------- 181.3/181.3 kB ? eta 0:00:00\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: urllib3<2.0 in c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.16)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2023.5.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.2)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6\n",
            "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
            "     ---------------------------------------- 83.9/83.9 kB 4.6 MB/s eta 0:00:00\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "     -------------------------------------- 151.7/151.7 kB 9.4 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (pyproject.toml): started\n",
            "  Building wheel for jax (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for jax: filename=jax-0.4.12-py3-none-any.whl size=1498562 sha256=57752ce5f0be44668f00ed5b54a84fddd252b0e097218e77f840fdbb91fd7d0d\n",
            "  Stored in directory: c:\\users\\monitsharma\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\22\\b1\\44\\2ff3c32e641f5419839cdb3fc38cfbd3418c22aba23e3399d0\n",
            "Successfully built jax\n",
            "Installing collected packages: libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, ml-dtypes, markdown, keras, h5py, grpcio, google-pasta, gast, absl-py, rsa, requests-oauthlib, pyasn1-modules, jax, astunparse, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
            "  Running setup.py install for wrapt: started\n",
            "  Running setup.py install for wrapt: finished with status 'done'\n",
            "Successfully installed absl-py-1.4.0 astunparse-1.6.3 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.20.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.2 h5py-3.8.0 jax-0.4.12 keras-2.12.0 libclang-16.0.0 markdown-3.4.3 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.3 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.3 tensorboard-data-server-0.7.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-intel-2.12.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 werkzeug-2.3.6 wheel-0.40.0 wrapt-1.14.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  DEPRECATION: wrapt is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
            "[notice] To update, run: C:\\Users\\monitsharma\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from pennylane.templates import RandomLayers\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting of the main hyper-parameters of the model\n",
        "=================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_epochs = 100   # Number of optimization epochs\n",
        "n_layers = 5    # Number of random layers\n",
        "n_train = 500    # Size of the train dataset\n",
        "n_test = 300     # Size of the test dataset\n",
        "\n",
        "SAVE_PATH = \"quanvolution/\" # Data saving folder\n",
        "PREPROCESS = True           # If False, skip quantum processing and load data from SAVE_PATH\n",
        "np.random.seed(135)           # Seed for NumPy random number generator\n",
        "tf.random.set_seed(135)       # Seed for TensorFlow random number generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading of the MNIST dataset\n",
        "============================\n",
        "\n",
        "We import the MNIST dataset from *Keras*. To speedup the evaluation of\n",
        "this demo we use only a small number of training and test images.\n",
        "Obviously, better results are achievable when using the full dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mnist_dataset = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n",
        "\n",
        "# Reduce dataset size\n",
        "train_images = train_images[:n_train]\n",
        "train_labels = train_labels[:n_train]\n",
        "test_images = test_images[:n_test]\n",
        "test_labels = test_labels[:n_test]\n",
        "\n",
        "# Normalize pixel values within 0 and 1\n",
        "train_images = train_images / 255\n",
        "test_images = test_images / 255\n",
        "\n",
        "# Add extra dimension for convolution channels\n",
        "train_images = np.array(train_images[..., tf.newaxis], requires_grad=False)\n",
        "test_images = np.array(test_images[..., tf.newaxis], requires_grad=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum circuit as a convolution kernel\n",
        "=======================================\n",
        "\n",
        "We follow the scheme described in the introduction and represented in\n",
        "the figure at the top of this demo.\n",
        "\n",
        "We initialize a PennyLane `default.qubit` device, simulating a system of\n",
        "$4$ qubits. The associated `qnode` represents the quantum circuit\n",
        "consisting of:\n",
        "\n",
        "1.  an embedding layer of local $R_y$ rotations (with angles scaled by a\n",
        "    factor of $\\pi$);\n",
        "2.  a random circuit of `n_layers`;\n",
        "3.  a final measurement in the computational basis, estimating $4$\n",
        "    expectation values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=4)\n",
        "# Random circuit parameters\n",
        "rand_params = np.random.uniform(high=2 * np.pi, size=(n_layers, 4))\n",
        "\n",
        "@qml.qnode(dev, interface=\"autograd\")\n",
        "def circuit(phi):\n",
        "    # Encoding of 4 classical input values\n",
        "    for j in range(4):\n",
        "        qml.RY(np.pi * phi[j], wires=j)\n",
        "\n",
        "    # Random quantum circuit\n",
        "    RandomLayers(rand_params, wires=list(range(4)))\n",
        "\n",
        "    # Measurement producing 4 classical output values\n",
        "    return [qml.expval(qml.PauliZ(j)) for j in range(4)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next function defines the convolution scheme:\n",
        "\n",
        "1.  the image is divided into squares of $2 \\times 2$ pixels;\n",
        "2.  each square is processed by the quantum circuit;\n",
        "3.  the $4$ expectation values are mapped into $4$ different channels of\n",
        "    a single output pixel.\n",
        "\n",
        "::: {.note}\n",
        "::: {.title}\n",
        "Note\n",
        ":::\n",
        "\n",
        "This process halves the resolution of the input image. In the standard\n",
        "language of CNN, this would correspond to a convolution with a\n",
        "$2 \\times 2$ *kernel* and a *stride* equal to $2$.\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def quanv(image):\n",
        "    \"\"\"Convolves the input image with many applications of the same quantum circuit.\"\"\"\n",
        "    out = np.zeros((14, 14, 4))\n",
        "\n",
        "    # Loop over the coordinates of the top-left pixel of 2X2 squares\n",
        "    for j in range(0, 28, 2):\n",
        "        for k in range(0, 28, 2):\n",
        "            # Process a squared 2x2 region of the image with a quantum circuit\n",
        "            q_results = circuit(\n",
        "                [\n",
        "                    image[j, k, 0],\n",
        "                    image[j, k + 1, 0],\n",
        "                    image[j + 1, k, 0],\n",
        "                    image[j + 1, k + 1, 0]\n",
        "                ]\n",
        "            )\n",
        "            # Assign expectation values to different channels of the output pixel (j/2, k/2)\n",
        "            for c in range(4):\n",
        "                out[j // 2, k // 2, c] = q_results[c]\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum pre-processing of the dataset\n",
        "=====================================\n",
        "\n",
        "Since we are not going to train the quantum convolution layer, it is\n",
        "more efficient to apply it as a \\\"pre-processing\\\" layer to all the\n",
        "images of our dataset. Later an entirely classical model will be\n",
        "directly trained and tested on the pre-processed dataset, avoiding\n",
        "unnecessary repetitions of quantum computations.\n",
        "\n",
        "The pre-processed images will be saved in the folder `SAVE_PATH`. Once\n",
        "saved, they can be directly loaded by setting `PREPROCESS = False`,\n",
        "otherwise the quantum convolution is evaluated at each run of the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if PREPROCESS == True:\n",
        "    q_train_images = []\n",
        "    print(\"Quantum pre-processing of train images:\")\n",
        "    for idx, img in enumerate(train_images):\n",
        "        print(\"{}/{}        \".format(idx + 1, n_train), end=\"\\r\")\n",
        "        q_train_images.append(quanv(img))\n",
        "    q_train_images = np.asarray(q_train_images)\n",
        "\n",
        "    q_test_images = []\n",
        "    print(\"\\nQuantum pre-processing of test images:\")\n",
        "    for idx, img in enumerate(test_images):\n",
        "        print(\"{}/{}        \".format(idx + 1, n_test), end=\"\\r\")\n",
        "        q_test_images.append(quanv(img))\n",
        "    q_test_images = np.asarray(q_test_images)\n",
        "\n",
        "    # Save pre-processed images\n",
        "    np.save(SAVE_PATH + \"q_train_images.npy\", q_train_images)\n",
        "    np.save(SAVE_PATH + \"q_test_images.npy\", q_test_images)\n",
        "\n",
        "\n",
        "# Load pre-processed images\n",
        "q_train_images = np.load(SAVE_PATH + \"q_train_images.npy\")\n",
        "q_test_images = np.load(SAVE_PATH + \"q_test_images.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us visualize the effect of the quantum convolution layer on a batch\n",
        "of samples:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = 4\n",
        "n_channels = 4\n",
        "fig, axes = plt.subplots(1 + n_channels, n_samples, figsize=(10, 10))\n",
        "for k in range(n_samples):\n",
        "    axes[0, 0].set_ylabel(\"Input\")\n",
        "    if k != 0:\n",
        "        axes[0, k].yaxis.set_visible(False)\n",
        "    axes[0, k].imshow(train_images[k, :, :, 0], cmap=\"gray\")\n",
        "\n",
        "    # Plot all output channels\n",
        "    for c in range(n_channels):\n",
        "        axes[c + 1, 0].set_ylabel(\"Output [ch. {}]\".format(c))\n",
        "        if k != 0:\n",
        "            axes[c, k].yaxis.set_visible(False)\n",
        "        axes[c + 1, k].imshow(q_train_images[k, :, :, c], cmap=\"gray\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below each input image, the $4$ output channels generated by the quantum\n",
        "convolution are visualized in gray scale.\n",
        "\n",
        "One can clearly notice the downsampling of the resolution and some local\n",
        "distortion introduced by the quantum kernel. On the other hand the\n",
        "global shape of the image is preserved, as expected for a convolution\n",
        "layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hybrid quantum-classical model\n",
        "==============================\n",
        "\n",
        "After the application of the quantum convolution layer we feed the\n",
        "resulting features into a classical neural network that will be trained\n",
        "to classify the $10$ different digits of the MNIST dataset.\n",
        "\n",
        "We use a very simple model: just a fully connected layer with 10 output\n",
        "nodes with a final *softmax* activation function.\n",
        "\n",
        "The model is compiled with a *stochastic-gradient-descent* optimizer,\n",
        "and a *cross-entropy* loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def MyModel():\n",
        "    \"\"\"Initializes and returns a custom Keras model\n",
        "    which is ready to be trained.\"\"\"\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training\n",
        "========\n",
        "\n",
        "We first initialize an instance of the model, then we train and validate\n",
        "it with the dataset that has been already pre-processed by a quantum\n",
        "convolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "q_model = MyModel()\n",
        "\n",
        "q_history = q_model.fit(\n",
        "    q_train_images,\n",
        "    train_labels,\n",
        "    validation_data=(q_test_images, test_labels),\n",
        "    batch_size=4,\n",
        "    epochs=n_epochs,\n",
        "    verbose=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to compare the results achievable with and without the quantum\n",
        "convolution layer, we initialize also a \\\"classical\\\" instance of the\n",
        "model that will be directly trained and validated with the raw MNIST\n",
        "images (i.e., without quantum pre-processing).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "c_model = MyModel()\n",
        "\n",
        "c_history = c_model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    validation_data=(test_images, test_labels),\n",
        "    batch_size=4,\n",
        "    epochs=n_epochs,\n",
        "    verbose=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results\n",
        "=======\n",
        "\n",
        "We can finally plot the test accuracy and the test loss with respect to\n",
        "the number of training epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(\"seaborn\")\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 9))\n",
        "\n",
        "ax1.plot(q_history.history[\"val_accuracy\"], \"-ob\", label=\"With quantum layer\")\n",
        "ax1.plot(c_history.history[\"val_accuracy\"], \"-og\", label=\"Without quantum layer\")\n",
        "ax1.set_ylabel(\"Accuracy\")\n",
        "ax1.set_ylim([0, 1])\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(q_history.history[\"val_loss\"], \"-ob\", label=\"With quantum layer\")\n",
        "ax2.plot(c_history.history[\"val_loss\"], \"-og\", label=\"Without quantum layer\")\n",
        "ax2.set_ylabel(\"Loss\")\n",
        "ax2.set_ylim(top=2.5)\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n",
        "==========\n",
        "\n",
        "1.  Maxwell Henderson, Samriddhi Shakya, Shashindra Pradhan, Tristan\n",
        "    Cook. \\\"Quanvolutional Neural Networks: Powering Image Recognition\n",
        "    with Quantum Circuits.\\\"\n",
        "    [arXiv:1904.04767](https://arxiv.org/abs/1904.04767), 2019.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
